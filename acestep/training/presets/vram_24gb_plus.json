{
  "_description": "High-capacity preset for GPUs with 24GB+ VRAM. Higher rank and batch size for best quality.",
  "adapter_type": "lora",
  "optimizer_type": "adamw",
  "scheduler_type": "cosine",
  "learning_rate": 1e-4,
  "batch_size": 2,
  "gradient_accumulation_steps": 2,
  "max_epochs": 100,
  "save_every_n_epochs": 10,
  "warmup_steps": 100,
  "max_grad_norm": 1.0,
  "gradient_checkpointing": false,
  "encoder_offloading": false,
  "torch_compile": false,
  "max_latent_length": 1500,
  "lora_r": 128,
  "lora_alpha": 256,
  "lora_dropout": 0.1,
  "cfg_dropout_prob": 0.15,
  "loss_weighting": "none",
  "auto_save_best_after": 200,
  "attention_type": "both"
}
