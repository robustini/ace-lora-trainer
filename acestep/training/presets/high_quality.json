{
  "_description": "High quality preset for long training runs. Higher rank, more epochs, Min-SNR weighting.",
  "adapter_type": "lora",
  "optimizer_type": "adamw",
  "scheduler_type": "cosine",
  "learning_rate": 5e-5,
  "batch_size": 1,
  "gradient_accumulation_steps": 4,
  "max_epochs": 1000,
  "save_every_n_epochs": 250,
  "warmup_steps": 200,
  "max_grad_norm": 1.0,
  "gradient_checkpointing": true,
  "encoder_offloading": false,
  "torch_compile": false,
  "max_latent_length": 1500,
  "lora_r": 128,
  "lora_alpha": 256,
  "lora_dropout": 0.05,
  "cfg_dropout_prob": 0.15,
  "loss_weighting": "min_snr",
  "snr_gamma": 5.0,
  "auto_save_best_after": 200,
  "early_stop_patience": 200,
  "attention_type": "both"
}
