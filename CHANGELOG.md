# Changelog

## 2026-02-25 â€” Trigger Word in LoRA Config, Captioner Config, Start Menu

### LoRA/LoKR: Trigger Word Saved in Adapter JSON
- The **activation tag** (trigger word) used during training is now automatically saved inside the adapter config JSON (`adapter_config.json` for LoRA, `lokr_config.json` for LoKR)
- Fields added: `trigger_word` and `tag_position` (prepend/append/replace)
- Extracted from preprocessed `.pt` file metadata at training start
- Saved in all outputs: checkpoints, best model, and final model
- The trigger word is also displayed in the training log at startup

### Captioner: Config Save/Load System
- New **Config (Save / Load Settings)** accordion in the Captioner UI
- Saves and restores all UI fields: model paths, input/output folders, activation tag, max tokens, lyrics/CSV options
- Config stored as `captioner_config.json` (default in project root)
- **Auto-load on startup**: if config file exists, all fields are restored automatically

### Start Menu: Interactive Launcher
- `start.bat` now shows an interactive menu to choose between:
  1. LoRA Trainer
  2. Captioner
  3. Both (Trainer + Captioner)
- Command-line arguments still work (e.g. `start.bat --mode caption`) and skip the menu

---

## 2026-02-24 â€” Captioner Performance Optimization + Detokenizer Fix

### Captioner: Single-Pass Caption + Metadata (~30-40% Faster)
- **New method:** `caption_and_analyze()` â€” combines audio description and metadata extraction (key, time signature, genre) into a **single model inference** instead of two separate calls
- **Before:** 2 inference calls per file (caption @ 512 tokens + metadata @ 150 tokens)
- **After:** 1 inference call per file (combined @ 600 tokens)
- BPM and duration still computed via librosa (no model needed, unchanged)
- Metadata parsed from the combined output using the existing `_parse_key()`, `_parse_genre()`, and time signature regex parsers
- Caption text is cleanly separated from metadata lines via regex split
- Fallback: if metadata parsing fails from combined output, caption text is mined as before

### Captioner: Audio Prefetch with Threading (~10-20% Faster)
- **New:** `ThreadPoolExecutor` prefetches the next audio file with librosa while the GPU processes the current file
- Overlaps I/O-bound audio loading with GPU-bound inference â€” eliminates idle CPU time between files
- Safe: GIL released during I/O, no shared mutable state

### Captioner: Reduced CUDA Synchronization Overhead (~5-10% Faster)
- Removed `torch.cuda.empty_cache()` from `_run_inference()` and `_transcriber_inference()` â€” these were called after **every single inference**, causing 2-4 unnecessary CUDA sync points per file
- Per-file cleanup in the main processing loop is retained (prevents OOM on large batches)
- Tensor deletion (`del inputs`, `del output_ids`, `del new_tokens`) still runs after every inference to free GPU memory immediately

### Combined Speedup
- **Estimated total:** ~40-55% faster for caption + metadata (without lyrics)
- **Estimated total:** ~25-35% faster for full pipeline (with lyrics â€” lyrics inference unchanged)
- Example: 50-file dataset previously ~5min â†’ now ~3min (caption + metadata only)

### Handler: Detokenizer Float32 Overflow Fix
- **Bug:** On long audio sequences, the detokenizer's self-attention QÂ·K dot products could exceed bfloat16 range (65504), producing Inf â†’ NaN that propagated through the entire ODE solver
- **Fix:** Detokenizer now runs in float32 temporarily during `_decode_audio_codes()`, then casts back to the model's dtype
- Added diagnostic logging for quantized output and detokenizer output (shape, min/max, NaN/Inf detection)

### Handler: Pre-Generation Tensor Diagnostics
- New diagnostic checkpoint before ODE generation: logs shape, dtype, min/max, mean/std, NaN/Inf status for all key tensors (src_latents, silence_latent, chunk_mask, text_hidden_states, precomputed_lm_hints_25Hz)
- Writes full diagnostics to `generation_diagnostics.json` for offline analysis

---

## 2026-02-23 â€” Upstream Sync: ACE-Step 1.5 Improvements

### Critical Bug Fixes (from upstream PRs)
- **torch.compile + PEFT crash guard (PR #640):** Auto-detects PEFT LoRA adapters and skips `torch.compile` to prevent crashes on PyTorch â‰¥2.7 + CUDA. Previously, enabling torch.compile with LoRA active would cause hard crashes
- **Symlink resolution in dataset paths (PR #648):** Uses `os.path.realpath()` instead of `os.path.normpath()` when resolving tensor file paths. Prevents false rejections when datasets are stored on symlinked/junction paths

### Two-Pass Preprocessing (Low VRAM)
- **New method:** `DatasetBuilder.preprocess_two_pass()` splits preprocessing into two sequential passes for GPUs with 8-12GB VRAM
  - **Pass 1 (~3GB):** VAE + Text Encoder â€” encodes audio to latents, tokenizes text/lyrics, saves intermediate `.tmp.pt` files
  - **Pass 2 (~6GB):** DIT Encoder â€” loads intermediates, runs model.encoder, saves final `.pt` files
  - Automatically offloads VAE + Text Encoder to CPU between passes
  - Temporary files cleaned up after completion
- **Why:** The single-pass pipeline requires all models simultaneously (~10-12GB). Two-pass mode enables preprocessing on 8GB GPUs

### GPU Monitoring
- **New module:** `acestep/training/gpu_monitor.py` with `GPUMonitor` class
  - Background threaded monitoring with configurable poll interval
  - VRAM alert threshold (default 92%) with callback support
  - Snapshot history with summary statistics (peak, average, utilization)
  - Auto-starts during Fabric training, logs summary at completion
- **New helpers:** `detect_gpu()` returns GPU name/driver/compute cap, `get_available_vram_mb()` returns free VRAM

### Cosine Restarts Scheduler
- **New scheduler option:** `scheduler_type = "cosine_restarts"` â€” cosine annealing with warm restarts
  - Periodically resets learning rate, which can help escape local minima on small LoRA datasets
  - First cycle = 1/4 of remaining steps, subsequent cycles double (T_mult=2)
  - Minimum LR = 1% of initial LR (same as cosine)

### VRAM Presets (Externalized JSON)
- **New directory:** `acestep/training/presets/` with 7 JSON preset files:
  - `vram_8gb.json` â€” rank 16, 8-bit optimizer, encoder offloading
  - `vram_12gb.json` â€” rank 32, 8-bit optimizer, encoder offloading
  - `vram_16gb.json` â€” rank 64, standard optimizer
  - `vram_24gb_plus.json` â€” rank 128, batch size 2
  - `recommended.json` â€” balanced defaults matching upstream
  - `quick_test.json` â€” rank 16, 10 epochs, fast iteration
  - `high_quality.json` â€” rank 128, 1000 epochs, min-SNR weighting
- **New APIs:** `list_presets()`, `load_preset(name)`, `apply_preset(config, lora_config, name)`, `auto_select_preset()` (auto-detects GPU and picks best preset)

### MLX Backend (Apple Silicon)
- **New module:** `acestep/mlx/` with infrastructure for Apple Silicon acceleration (2-3x faster than MPS)
  - `__init__.py` â€” `is_mlx_available()` detection (macOS + arm64 + mlx import)
  - `convert.py` â€” weight conversion from PyTorch to MLX format (Conv1d axis swaps, weight_norm fusion, rotary embedding skipping)
  - `engine.py` â€” `MLXInferenceEngine` class with model loading, weight caching, and generation API surface
  - Weight conversion and caching is fully implemented; full DiT/VAE model porting is in progress

### Data Preparation Scripts (Captioner)
- **WhisperTranscriber** â€” Transcribe lyrics via OpenAI Whisper API with word-level timestamps and intelligent line breaking for CJK/Latin scripts. Batch directory processing with sidecar `.lyrics.txt` output
- **ElevenLabsTranscriber** â€” Transcribe lyrics via ElevenLabs Scribe API. Same word-level timestamp approach as Whisper, filters for word-type entries only
- **GeminiCaptioner** â€” Full audio analysis via Google Gemini API. Generates caption, lyrics, genre, BPM, key, time signature in structured JSON. Supports large files (>20MB) via file upload API. Batch processing with sidecar `.caption.txt` and `.lyrics.txt` files

## 2026-02-22b â€” Min-SNR Loss Weighting, NaN Detection, Audio Normalization

### Min-SNR Loss Weighting (from Side-Step / Hang et al. 2023)
- **New option:** `loss_weighting = "min_snr"` in TrainingConfig â€” applies Min-SNR-gamma weighting to per-timestep loss
- **How it works:** Computes `SNR(t) = (1-t)^2 / t^2` for flow-matching timesteps, then weights loss by `clamp(SNR, max=gamma) / SNR`. Easy (high-SNR, low-t) timesteps contribute less, while noisy timesteps are capped at `gamma`
- **Configurable:** `snr_gamma` parameter (default 5.0, matching the paper) controls the clamping value
- **UI:** New "Loss Weighting" dropdown and "SNR Gamma" slider in the Advanced: Timestep & CFG section
- **Compatible with:** Both Fabric and basic training loops

### NaN / Inf Detection & Auto-Halt
- **New feature:** Automatically detects NaN or Infinity losses during training and halts after N consecutive bad batches
- **Configurable:** `nan_detection_max` parameter (default 10, 0=disabled). Set via "NaN Halt Threshold" in the UI
- **Diagnostics:** When training halts, prints detailed diagnostics: consecutive count, total count, last epoch/batch, possible causes (LR too high, gradient explosion, data corruption, fp16 overflow)
- **Graceful recovery:** Bad batches are skipped (gradients zeroed), training continues if within threshold. Counter resets on any good batch
- **Both loops:** Implemented in both Fabric and basic (non-Fabric) training loops

### Audio Normalization During Preprocessing
- **New option:** Normalize audio loudness before VAE encoding during tensor preprocessing
- **Modes:**
  - `none` (default) â€” raw audio as-is
  - `peak` â€” peak-normalize to -1.0 dBFS (loudest sample = 1.0)
  - `lufs` â€” loudness-normalize to -14 LUFS (broadcast standard)
  - `peak_lufs` â€” both: peak-normalize first, then LUFS-normalize
- **Why:** Inconsistent volume levels across training samples can cause the model to learn volume artifacts instead of musical features. Normalizing audio before encoding ensures consistent latent representations
- **UI:** New "Audio Normalization" dropdown in the preprocessing section
- **Anti-clipping:** LUFS normalization includes automatic clip protection (peaks are capped at 1.0)
- **Manifest:** Normalization mode is recorded in the manifest.json `preprocessing` section for reproducibility

---

## 2026-02-22 â€” Side-Step Interop, Language Override, Bug Fixes

### Side-Step Config Interoperability
- **Import/export** training configs compatible with [Side-Step](https://github.com/koda-dernet/Side-Step) â€” the complementary LoRA training toolkit by [@koda-dernet](https://github.com/koda-dernet)
- **Import:** Load any Side-Step preset JSON (recommended, vram_8gb, high_quality, etc.) and automatically map all 17 training parameters to the ace-lora-trainer UI
- **Export:** Save current training settings as a Side-Step-compatible preset JSON that works directly in Side-Step's `--preset` flag
- **Auto-detection:** Import function automatically identifies Side-Step presets vs ace-lora-trainer profiles
- **Bidirectional field mapping:** Handles naming differences (e.g., `rank` vs `lora_rank`, `adamw8bit` vs `AdamW 8-bit`, `cfg_ratio` vs `cfg_dropout_prob`)
- **New UI section:** "Side-Step Interop" panel inside the Profiles accordion with browse/import/export buttons

### Language Override for AI Auto-Labeling
- **New dropdown** in the auto-label section: choose a specific language instead of relying on the 5Hz LM model's auto-detection (which was often random/incorrect for non-English lyrics)
- **40 languages supported:** English, Spanish, French, German, Italian, Portuguese, Russian, Chinese, Japanese, Korean, Arabic, Hindi, and 28 more
- **How it works:** The LLM still runs full auto-detection for captions/BPM/key/etc., then the detected language is overridden with the user's selection on all non-instrumental samples
- **Full stack:** UI dropdown, `label_sample()` and `label_all_samples()` in dataset_builder, API server endpoint

### Fix: Training won't start â€” tensor files not found (Issue #7)
- **Bug:** After preprocessing, training failed with "0 samples cached in RAM" because the manifest stored absolute paths. Moving the tensor directory or running from a different location broke all path references
- **Root cause:** `dataset_builder.py` saved full absolute paths (e.g., `D:\datasets\tensor.pt`) in `manifest.json`. The data module loader expected these exact paths to exist
- **Fix (manifest writer):** Now stores relative paths (just filenames) in both `samples` and `sample_details` arrays for full portability
- **Fix (manifest reader):** Resolves relative paths against the tensor directory. Added fallback: if an absolute path doesn't exist, tries to find the file by basename in the tensor directory â€” handles legacy manifests with absolute paths

### Fix: Sample inference device mismatch during training (Issue #2 follow-up)
- **Bug:** `RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!` when sample inference was enabled with encoder offloading
- **Root cause:** With encoder offloading active, VAE/text_encoder/encoder are on CPU during training. `generate_music()` needs them on GPU but nothing moved them back
- **Fix:** Before generating samples, explicitly move `vae`, `text_encoder`, and `encoder` to the training device (GPU). After generation, the existing cleanup code moves them back to CPU if offloading is enabled

---

## 2026-02-18a â€” Sample Inference During Training + Fix Gradient Checkpointing Crash

### Auto sample inference during training (Issue #2)
- **New feature:** Generate test audio at regular epoch intervals so you can hear how the model evolves during training â€” loss curves alone can't tell you if the model sounds good
- **Configurable:** Enable/disable toggle, sample every N epochs, prompt, lyrics, BPM, key, time signature, duration, seed
- **Multiple LoRA strengths:** Comma-separated (e.g., "0.5, 1.0, 1.5") â€” generates one sample per strength per interval
- **Inference params:** Steps, guidance scale, shift (0 = use training config defaults)
- **VRAM-safe:** Graceful OOM handling â€” if sample generation fails due to VRAM, logs a warning and continues training
- **Output:** Saved to `<output_dir>/samples/epoch_<N>/strength_<S>.wav`
- **New UI section:** "ðŸŽµ Sample Inference During Training" accordion in the training tab

### Fix gradient checkpointing crash (Issue #3)
- **Bug:** Training crashed immediately with `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when gradient checkpointing was enabled
- **Root cause:** PEFT + gradient checkpointing requires `enable_input_require_grads()` on the PEFT model before `gradient_checkpointing_enable()`. Without it, checkpointed layers see inputs without `requires_grad=True` and produce a loss with no grad_fn
- **Fix:** Added `enable_input_require_grads()` call before gradient checkpointing is enabled

---

## 2026-02-17a â€” Fix Captioner OOM, Key/Scale, and Checkpoint Path

### Fix captioner OOM on 32GB GPUs
- **Bug:** Batch captioning (60+ files) caused CUDA OOM even on 32GB GPUs (5090). The `_run_inference()` and `_transcriber_inference()` methods accumulated input tensors and KV-cache across multiple calls without freeing GPU memory
- **Fix:** Added `try/finally` cleanup in both inference methods â€” `inputs`, `output_ids`, and `new_tokens` are now `del`'d and `torch.cuda.empty_cache()` runs after **every single inference call**, not just between files
- **Fix:** `transcribe_lyrics()` now accepts pre-loaded `audio_array` to avoid redundant audio reloading during batch processing

### Fix key/scale extraction still failing
- **Bug:** Key extraction returned empty for most files despite genre working fine
- **Fix:** Improved metadata prompt to be more natural (Qwen responds better to questions than rigid format instructions)
- **Fix:** Expanded `_parse_key()` with 7 cascading regex patterns including: "is D minor", "is in C# major", "in the key of A minor", shorthand "Dm"/"Am", standalone note+mode, and key-related word proximity matching
- **Fix:** Added unicode sharp/flat normalization (â™¯â†’#, â™­â†’b) before regex matching
- **Fix:** Improved time signature fallback to match "4/4", "3/4" anywhere in text

### Fix checkpoint path â€” models re-downloaded despite custom directory
- **Bug:** When users pointed to an existing ACE-Step checkpoints folder (e.g., their main ACE-Step install), the trainer ignored it and re-downloaded ~20GB of models to its own `./checkpoints` directory
- **Root cause:** `initialize_service()` always derived `checkpoint_dir` from `_get_project_root()` before checking if models exist. The custom dir override only happened AFTER the download checks had already run
- **Fix:** Added `custom_checkpoint_dir` parameter to `initialize_service()` â€” now checks custom dir for models BEFORE falling back to download. LLM handler also searches both custom and default directories

---

## 2026-02-16c â€” Fix Auto-Label + Split Audio

### Fix auto-label "Model not initialized" after LLM download
- **Bug:** After downloading & loading the LLM, clicking "Auto-Label All" failed with "Failed to convert audio to codes: Model not initialized" because `convert_src_audio_to_codes()` needs the VAE (loaded only with lazy init, not at startup)
- **Fix:** `auto_label_samples()` now calls `dit_handler.ensure_models_loaded()` to auto-load VAE + text encoder before labeling â€” same pattern as preprocessing

### Fix split audio `[WinError 2]` â€” ffmpeg not found
- **Bug:** Split audio used `pydub.AudioSegment` which requires `ffmpeg`/`ffprobe` binaries on the system PATH
- **Fix:** Replaced `pydub` with `torchaudio` + `soundfile` (both already dependencies) â€” no external binary needed

---

## 2026-02-16b â€” Captioner: Fix OOM + Missing Key/Genre

### Fix OOM on large batches (60+ files)
- Audio is now loaded **once per file** and reused for caption, metadata, and lyrics (was loading 2-3x)
- Added `torch.cuda.empty_cache()` between files to release KV cache and prevent VRAM fragmentation
- Reduces peak VRAM usage significantly for batch captioning

### Fix missing Key Scale and Genre
- Improved metadata extraction prompt for more reliable structured output from Qwen
- Extracted key/genre parsing into reusable `_parse_key()` / `_parse_genre()` static methods
- Added **caption text mining** as fallback: if the structured metadata call fails to extract key or genre, the caption description is searched (e.g., "electronic pop track in D minor")
- Expanded genre keyword list with compound genres (synth-pop, trip-hop, drum and bass, etc.)
- Key parser now handles more formats: "in D minor", "the key of A", shorthand "Dm", note-only "D"

---

## 2026-02-16a â€” Optional LLM Download + Fix `--mode both`

### Optional LLM Download for Auto-Labeling
The LLM for AI auto-labeling is now fully optional with on-demand download.

- Added **"Download & Enable AI Labeling (~3.5 GB)"** button in the Dataset tab's auto-label section
- LLM is no longer required at startup â€” users can download it when needed
- Auto-label error message now directs users to either download the LLM or use CSV/manual labeling
- Info box recommends standalone captioning tools for higher quality captions (importable via CSV)

### Fix `--mode both` captioner not starting
- **Bug:** `python launch.py --mode both` launched the captioner UI in a daemon thread, which could silently crash due to Gradio event loop conflicts between two apps in the same process
- **Fix:** Captioner now runs as a **separate subprocess**, which isolates each Gradio app's event loop and makes both UIs reliable. The captioner process is automatically terminated when the training UI exits

---

## 2026-02-15f â€” Fix Preprocessing Crash (VAE not loaded)

**Bug:** Preprocessing failed with `AttributeError: 'NoneType' object has no attribute 'dtype'` because lazy loading only loaded the DiT model at initialization, leaving VAE and text encoder as `None`. Preprocessing requires both.

**Fix:**
- `lora_training_ui.py`: `preprocess_dataset()` now calls `dit_handler.ensure_models_loaded()` before preprocessing to auto-load VAE + text encoder if needed
- `dataset_builder.py`: Added explicit guard checks for `vae is None` and `text_encoder is None` with clear error messages instead of cryptic `AttributeError`

Fixes [#1](https://github.com/Estylon/ace-lora-trainer/issues/1).

---

## 2026-02-15e â€” Adapter-Aware GPU Presets (LoRA vs LoKr)

GPU presets now differentiate between LoRA and LoKr training parameters.

**Key LoKr preset differences vs LoRA:**
- **Learning rate:** 3e-4 (vs 1e-4) â€” Kronecker factorization benefits from higher LR
- **Gradient accumulation:** 4-8x (vs 1-2x) â€” smooths noisy gradients from smaller effective batch
- **Epochs:** 500 (vs 800-1000) â€” LoKr converges faster
- **Optimizer:** AdamW (vs Prodigy) â€” standard optimizer, more predictable with LoKr
- **Dropout:** 0.0 (vs 0.1) â€” LoKr already has implicit regularization via Kronecker structure
- **Early stop patience:** 50 (vs 80) â€” faster convergence = detect plateau earlier
- **Best-model warmup:** 100 (vs 200) â€” start tracking best MA5 sooner

**How it works:**
- Selecting a GPU preset applies the LoRA or LoKr sub-preset based on the current Adapter Type selection
- Switching Adapter Type (LoRAâ†”LoKr) while a preset is active automatically re-applies the correct sub-preset
- LoKr-specific fields (Factor, Linear Dim, Linear Alpha, Decompose Both, Tucker, Dropout) are also set by presets
- "Custom" preset still leaves everything unchanged

### Other fixes
- **GPU VRAM status not showing** â€” Fixed `total_mem` â†’ `total_memory` typo in `get_vram_info()`
- **Final model uses best MA5** â€” Training completion now copies the best MA5 checkpoint as `final/` instead of saving last-epoch weights

---

## 2026-02-15c â€” Custom Checkpoints Folder

Added the ability to select a custom folder for model checkpoints in the Service tab.

- **New "Checkpoints Folder" field** â€” Textbox + browse button (`ðŸ“‚`) above the model dropdown. Point it to any folder containing `acestep-v15-*` model directories.
- **Auto-refresh** â€” Changing the folder automatically rescans and updates the Model Checkpoint dropdown.
- **Refresh button** now also respects the custom folder when rescanning.
- **`initialize_service()`** â€” Resolves the selected model from the custom folder and overrides the handler's checkpoint dir so `_load_dit()` finds it correctly.
- Leave the field empty to use the default `checkpoints/` locations as before.

---

## 2026-02-15b â€” LoKr Training Speed Fix + lycoris-lora Required

### Critical Fix: LoKr Training 10-50x Slower Than LoRA

**Root cause:** The LyCORIS library creates adapter parameters on **CPU** regardless of where the decoder lives (GPU). Our code never explicitly relocated these parameters to GPU after injection. Every forward pass during LoKr training triggered CPUâ†’GPU data transfers, making training catastrophically slow (~300s/epoch for 32 samples vs ~10s for LoRA).

**How we found it:** The upstream [ACE-Step 1.5](https://github.com/ace-step/ACE-Step-1.5) V2 training module (`training_v2/fixed_lora_module.py`) has an explicit `.to(self.device)` call after LoKr injection that our V1-based trainer was missing.

**Files changed:**
- **`acestep/training/lokr_utils.py`** â€” After `lycoris_net.apply_to()`, explicitly move `lycoris_net` and `model.decoder` to the decoder's device so all parameters (base + adapter) are co-located before training begins.
- **`acestep/training/trainer.py`** â€” In both `_train_with_fabric()` and `_train_basic()`, ensure `lycoris_net` params match training dtype and device before Fabric setup / optimizer creation. Removed the redundant `fabric.setup_module(decoder)` call that was double-wrapping the decoder.

### lycoris-lora Now a Required Dependency

- `requirements.txt` â€” Changed `lycoris-lora>=2.0.0` from optional (commented) to required. LoKr is a first-class adapter type alongside LoRA.

---

## 2026-02-15 â€” Lazy/On-Demand Model Loading

Reduced idle VRAM usage and startup time by loading models only when needed.

- **`initialize_service(lazy=True)`** â€” new mode that stores config and downloads models but defers weight loading. The server/UI is ready immediately.
- **`ensure_models_loaded()`** â€” thread-safe lazy loader that loads all models (DiT + VAE + text encoder) on first generation request. Uses double-check locking for safe concurrent access.
- **`ensure_dit_loaded()`** â€” loads only the DiT model for training workflows, skipping VAE and text encoder (~3GB VRAM saved).
- **Training UI** now calls `ensure_dit_loaded()` instead of loading all models â€” VAE and text encoder are never used during training.
- **`generate_music()`** auto-triggers `ensure_models_loaded()` if models aren't loaded yet.
- **Pinokio API server** uses lazy init for all DiT handlers (primary + secondary + tertiary) â€” server starts in <5s instead of 30-120s. `/health` endpoint now reports `models_loaded` and `llm_loaded` status.

---

## 2026-02-14b â€” LoKr Support, Upstream Optimizations & Bug Fixes

### LoKr (Low-Rank Kronecker) Training Support

Added full LoKr adapter training as an alternative to LoRA, powered by the [LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS) library.

**New files:**
- `acestep/training/lokr_utils.py` â€” LoKr injection, save/load/merge utilities
- `acestep/training/configs.py` â€” New `LoKRConfig` dataclass

**What changed:**
- **Training UI** â€” New "Adapter Type" radio selector (LoRA / LoKr) with conditional parameter panels. LoKr settings: Factor (-1 = auto), Linear Dim (10000 = auto), Linear Alpha, Decompose Both, Use Tucker, Dropout.
- **Trainer** â€” `PreprocessedLoRAModule` and `LoRATrainer` now branch on `adapter_type` for injection, save, load, merge, gradient clipping, and Fabric setup. The training loop (flow matching, timestep sampling, CFG dropout) is identical for both adapters.
- **Merge tab** â€” Auto-detects adapter type from checkpoint files (`adapter_config.json` = PEFT LoRA, `lokr_config.json` = LyCORIS LoKr).
- **Inference loader** (`handler.py`) â€” `load_lora()` auto-detects and loads both LoRA (PEFT) and LoKr (LyCORIS) adapters. `unload_lora()` cleans up both adapter types.

**LoKr vs LoRA:**
- LoKr uses Kronecker product factorization â€” often more parameter-efficient at similar expressiveness
- LoKr is experimental and requires `lycoris-lora>=2.0.0` (optional dependency, commented in `requirements.txt`)
- LoRA remains the default and recommended option

### Upstream ACE-Step 1.5 Optimizations Integrated

Integrated key improvements from the upstream [ACE-Step 1.5](https://github.com/ace-step/ACE-Step-1.5) repository:

**PR #499 â€” LoRA Memory Bloat Fix (70% VRAM reduction on load/unload)**
- Replaced `copy.deepcopy(model.decoder)` with CPU `state_dict()` backup for the base decoder
- Loading a LoRA/LoKr adapter no longer doubles VRAM usage (~10-15GB saved)
- Unloading restores from CPU state_dict + calls `torch.cuda.empty_cache()`
- VRAM usage is logged before/after load and unload operations

**PR #554 â€” Opus and AAC Output Formats**
- `AudioSaver` now supports `opus` and `aac` formats via ffmpeg backend
- Added to format validation, extension handling, and convert functions
- Gradio UI dropdown updated with new format options

**PR #492 â€” Batch LLM Load/Offload Optimization**
- `_load_model_context()` made reentrant: if model is already on GPU, nested calls are no-ops
- Batch loop in `_run_pt()` wrapped in a single `_load_model_context()` so the LLM loads once and offloads once per batch (was N loads + N offloads before)

**PR #493 â€” Phase-Aware max_new_tokens / Progress Bar Fix**
- New `_compute_max_new_tokens()` method replaces inline calculations
- In "codes" phase, buffer is +10 tokens (not +500) since constrained decoder forces exact EOS
- Fixes progress bar appearing to stall at ~66% during codes generation

### Bug Fix: Checkpoint Discovery for Pinokio Layout

- `get_checkpoints_dir()` now searches both `ACE-Step-1.5/checkpoints/` and `checkpoints/` directories
- New `_get_all_checkpoint_dirs()` scans all known checkpoint locations with deduplication
- New `resolve_checkpoint_path()` resolves checkpoint names to full paths across all directories
- Fixes empty "Model Checkpoint" dropdown when running under Pinokio (where base models are symlinked to `ACE-Step-1.5/checkpoints/`, not `checkpoints/`)

---

## 2026-02-14 â€” Training Quality Fix: Corrected Timestep Sampling & CFG Dropout

### Background â€” How We Found These Issues

While reviewing the upstream [ACE-Step 1.5 repository](https://github.com/ace-step/ACE-Step-1.5), we noticed that the **Side-Step training v2** module ([PR #478](https://github.com/ace-step/ACE-Step-1.5/pull/478) by [@koda-dernet](https://github.com/koda-dernet)) was merged on Feb 13. This module introduced a completely new `acestep/training_v2/` directory with corrected training algorithms.

After a thorough analysis comparing our `acestep/training/trainer.py` (v1) against the new `trainer_fixed.py` (v2), we identified **4 critical correctness issues** in our training pipeline that were degrading LoRA quality. These issues existed in the original training code that we inherited, not in our own additions.

The key insight from Side-Step's code is that the original v1 trainer used timestep sampling and CFG dropout strategies designed for **inference**, not for **LoRA training**. The v2 trainer corrects these to match the model's actual **pre-training** distribution.

### Critical Fix 1: Timestep Sampling â€” Discrete â†’ Logit-Normal

**Problem:** Our trainer (v1) used a fixed set of 8 discrete timesteps for turbo models, sampled uniformly from `[1.0, 0.95, 0.9, 0.83, 0.75, 0.64, 0.5, 0.3]`. This schedule was designed for the turbo model's 8-step **inference** pipeline, not for training.

**Why it matters:** During pre-training, ACE-Step uses a **logit-normal distribution** (`t = sigmoid(N(Î¼, Ïƒ))` with `Î¼=-0.4, Ïƒ=1.0`) to sample timesteps. By training LoRA with only 8 discrete timesteps, we were teaching the model to only denoise at those specific noise levels, creating a distribution mismatch with the full continuous spectrum the model actually uses.

**Fix:** Replaced `sample_discrete_timestep()` and `sample_continuous_timestep()` with a single `sample_logit_normal_timestep()` that faithfully reproduces the model's own `sample_t_r()` function (from `modeling_acestep_v15_turbo.py` lines 169-194). This is used for **all** model variants (turbo and base).

**Source:** `acestep/training_v2/timestep_sampling.py:sample_timesteps()` in [ACE-Step 1.5 PR #478](https://github.com/ace-step/ACE-Step-1.5/pull/478)

### Critical Fix 2: CFG Dropout â€” Zeroed Embeddings â†’ Learned Null Embedding

**Problem:** When applying classifier-free guidance (CFG) dropout during training, our v1 code **zeroed out** the encoder hidden states for dropped samples. Additionally, CFG dropout was only applied to base models, not turbo.

**Why it matters:** The model was pre-trained with a specific **learned null/unconditional embedding** (`model.null_condition_emb`), not zero vectors. Zeroing out embeddings creates an artificial input the model never saw during pre-training, producing incorrect gradient signals. Furthermore, the original pre-training applied CFG dropout to all model variants, not just base.

**Fix:** New `apply_cfg_dropout()` function that:
1. Uses `model.null_condition_emb` (the model's learned unconditional embedding) instead of zeros
2. Is applied to **all model types** (turbo + base), matching pre-training behavior
3. Default dropout probability changed from 0.1 to 0.15 (matching v2 defaults)

**Source:** `acestep/training_v2/timestep_sampling.py:apply_cfg_dropout()` and `trainer_fixed.py` lines referencing `model.null_condition_emb` in [ACE-Step 1.5 PR #478](https://github.com/ace-step/ACE-Step-1.5/pull/478)

### Critical Fix 3: Loss Reporting â€” Corrected Arithmetic

**Problem:** When gradient accumulation > 1, our reported loss values were divided by `gradient_accumulation_steps` (because each micro-batch loss is already divided by G before backward). The logged/displayed loss was `1/G` of the actual per-sample loss.

**Fix:** Corrected loss computation: `avg_loss = accumulated_loss * gradient_accumulation_steps / accumulation_step` to report the true per-sample loss value.

**Note:** If you compare loss curves from before and after this fix, post-fix values will appear higher by a factor of G. The underlying training is identical â€” only the reported number changed.

### Critical Fix 4: End-of-Epoch Remainder Flushing

**Problem:** When the number of batches per epoch is not evenly divisible by `gradient_accumulation_steps`, the leftover accumulated gradients at the end of each epoch were silently dropped. For example, with 17 songs and gradient_accumulation=4, the last 1 sample's gradients were thrown away every epoch.

**Fix:** Added explicit remainder flushing at epoch boundaries â€” if there are accumulated gradients when the epoch ends, an optimizer step is performed to use them.

### New UI Parameters

- **Timestep Î¼ (mu)** â€” Logit-normal mean. Default -0.4 (bias toward cleaner data, matching pre-training). Available in Advanced accordion.
- **Timestep Ïƒ (sigma)** â€” Logit-normal standard deviation. Default 1.0. Available in Advanced accordion.
- **CFG Dropout** â€” Moved from base-model-only to a general parameter. Default 0.15 (15%). Now applies to all model types.

### Other Improvements
- **Per-epoch VRAM cache clearing** â€” Calls `torch.cuda.empty_cache()` at end of each epoch to reduce VRAM fragmentation
- Training start message now shows timestep distribution parameters: `logit-normal(Î¼=-0.4, Ïƒ=1.0)`

### What We Kept (Our Features Not in v2)
The v2 Side-Step trainer removes several features that we intentionally keep because they improve the training experience:
- âœ… Early stopping with MA5 smoothed loss tracking
- âœ… Auto-save best model after warmup period
- âœ… torch.compile support
- âœ… VRAM monitor and pre-training GPU check
- âœ… GPU presets for different hardware tiers

---

## 2026-02-13 â€” Initial Release

First public release of the ACE-Step LoRA Trainer + Captioner as a standalone project, separated from the Pinokio launcher.

### Repository Setup
- **Initial commit** â€” Extracted core training modules (`acestep/training/`), model handler, LLM inference, captioner, and Gradio UIs from the Pinokio-based ACE-Step project into a self-contained standalone repository
- 20,000+ lines of code across 26 files
- Published to GitHub at `github.com/Estylon/ace-lora-trainer`

### Bug Fixes
- **Fixed ACESTEP_PATH pointing to non-existent directory** â€” Was looking for `ACE-Step-1.5/` which doesn't exist in standalone repo. Changed to repo root so checkpoint discovery works correctly
- **Fixed captioner/transcriber hardcoded Pinokio paths** â€” Changed `DEFAULT_MODEL_PATH` from `D:/pinokio/drive/.../acestep-captioner` to HuggingFace model ID `ACE-Step/acestep-captioner` so models auto-download on first use
- **Fixed captioner rejecting HuggingFace model IDs** â€” `os.path.exists()` was blocking valid HF model IDs like `ACE-Step/acestep-captioner`. Added HF ID detection (org/model format) to allow them through to `from_pretrained()`
- **Fixed torchao breaking Windows installs** â€” torchao has no Windows wheels and requires `torch.compile` + Triton (neither available on Windows). Made it Linux-only in `requirements.txt` since it's only needed for INT8/FP8 quantization, not LoRA training
- **Fixed RTX 3060/4060 GPU preset** â€” Changed optimizer from `adamw8bit` to `adafactor` and scheduler from `cosine` to `constant_with_warmup`. Adafactor uses near-zero optimizer state memory, making it viable for 8GB VRAM where AdamW 8-bit still OOMs

### Dependency Fixes
- Added missing `pydub` (audio splitting) and `diskcache` (caching) to requirements
- Removed unused `xxhash` dependency
- Added missing `acestep/genres_vocab.txt` (178K genre vocabulary file needed for caption generation)
- Updated `.gitignore` with `.cache/`, `lora_projects/`, `datasets/*.json`

### New Features

#### Install & Launch Scripts
- **`install.bat`** (Windows) and **`install.sh`** (Linux/Mac) â€” One-click installer that creates virtual environment, installs all dependencies via `uv pip` (with pip fallback), and verifies critical packages (PyTorch, PEFT, Lightning, Gradio, Prodigy, Transformers)
- **`start.bat`** (Windows) and **`start.sh`** (Linux/Mac) â€” One-click launcher that activates venv and runs the trainer
- **Environment checks in `launch.py`** â€” Detects if running without a virtual environment or with missing packages (especially PEFT) and warns before proceeding. This prevents the silent failure where training falls back to full fine-tuning without LoRA (~35x larger checkpoints, much slower)

#### VRAM Monitor
- **Real-time GPU status** in the Training tab showing GPU name, VRAM used/free/total with color-coded indicator (green/yellow/red)
- **Other process detection** â€” Uses `nvidia-smi` to list other processes occupying the GPU (e.g., ACE-Step API) with their VRAM usage
- **Pre-training VRAM check** â€” When clicking "Start Training", automatically checks for insufficient free VRAM and warns about other GPU processes
- **Refresh button** for manual VRAM status updates

### Documentation
- **`TRAINING_GUIDE.md`** â€” Comprehensive ~950-line training guide covering complete workflow (7 steps from captioning to merge), all parameters with defaults/ranges/explanations, all 4 GPU presets with full parameter tables, optimizer/scheduler comparison tables, disk space requirements, security audit section, and troubleshooting guide
- **`LICENSE`** â€” Apache License 2.0
- **`README.md`** â€” Updated with accurate GPU preset table (including scheduler column), auto-download model documentation, simplified install instructions using new scripts, and links to training guide

### Architecture Notes
The standalone trainer is **100% self-sufficient** â€” all model loading and inference happens in-process via PyTorch/HuggingFace Transformers. No external API server is required. The Gradio UI handles everything: model initialization, dataset building, auto-labeling, preprocessing, training, and LoRA merging.
